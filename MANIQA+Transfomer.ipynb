{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7691610e-65b8-4955-8a4d-27fca9b76373",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bb06abd-6a59-4298-976d-f2cd487e9ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import timm\n",
    "from MANIQA import *\n",
    "from MANIQA2Transformer import *\n",
    "import torch.nn.utils as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26806a2c-9755-45f2-b945-4cdc26dc4165",
   "metadata": {},
   "source": [
    "## Hyperparameter Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaffb268-9a47-45da-942b-f6b60b52b7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE':224,\n",
    "    'EPOCHS': 100, #Your Epochs,\n",
    "    'LR': 3e-4, #Your Learning Rate,\n",
    "    'BATCH_SIZE': 128, #Your Batch Size,\n",
    "    'SEED':41\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b0fbf6-43b7-4d09-81da-149147f5fa44",
   "metadata": {},
   "source": [
    "## Fixed Random-Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a0e6a64-4f23-4813-9426-e0b56ce797ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcae6476-b1cc-434b-8f86-5149a283858d",
   "metadata": {},
   "source": [
    "## Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd25d3e2-f7c5-4f05-b6b0-d90f825b975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.RandAugment(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.RandomErasing(),\n",
    "            transforms.Normalize(mean=[0.5,0.5,0.5],\n",
    "                        std=[0.5,0.5,0.5])\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx]['img_path']\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        img = self.transform(img)\n",
    "        \n",
    "        # mos column 존재 여부에 따라 값을 설정\n",
    "        mos = float(self.dataframe.iloc[idx]['mos']) if 'mos' in self.dataframe.columns else 0.0\n",
    "        comment = self.dataframe.iloc[idx]['comments'] if 'comments' in self.dataframe.columns else \"\"\n",
    "\n",
    "        \n",
    "        return img, mos, comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e1f800d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_name</th>\n",
       "      <th>img_path</th>\n",
       "      <th>mos</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41wy7upxzl</td>\n",
       "      <td>./train/41wy7upxzl.jpg</td>\n",
       "      <td>5.569231</td>\n",
       "      <td>the pink and blue really compliment each other...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ygujjq6xxt</td>\n",
       "      <td>./train/ygujjq6xxt.jpg</td>\n",
       "      <td>6.103175</td>\n",
       "      <td>love rhubarb! great colors!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wk321130q0</td>\n",
       "      <td>./train/wk321130q0.jpg</td>\n",
       "      <td>5.541985</td>\n",
       "      <td>i enjoy the textures and grungy feel to this. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>w50dp2zjpg</td>\n",
       "      <td>./train/w50dp2zjpg.jpg</td>\n",
       "      <td>6.234848</td>\n",
       "      <td>i like all the different colours in this pic, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>l7rqfxeuh0</td>\n",
       "      <td>./train/l7rqfxeuh0.jpg</td>\n",
       "      <td>5.190476</td>\n",
       "      <td>i love these critters, just wish he was a litt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74563</th>\n",
       "      <td>zbevd0lyox</td>\n",
       "      <td>./train/zbevd0lyox.jpg</td>\n",
       "      <td>5.926108</td>\n",
       "      <td>perfect balance here, in this soft serene image.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74564</th>\n",
       "      <td>w26yu6ee60</td>\n",
       "      <td>./train/w26yu6ee60.jpg</td>\n",
       "      <td>5.966346</td>\n",
       "      <td>very nice indeed. the sharpness and contrast a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74565</th>\n",
       "      <td>a1pts9zzdx</td>\n",
       "      <td>./train/a1pts9zzdx.jpg</td>\n",
       "      <td>5.718447</td>\n",
       "      <td>nice tones and color for balance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74566</th>\n",
       "      <td>pzbubeo03l</td>\n",
       "      <td>./train/pzbubeo03l.jpg</td>\n",
       "      <td>6.007843</td>\n",
       "      <td>i like the bold colors. nice sharp image.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74567</th>\n",
       "      <td>8c0klk5ule</td>\n",
       "      <td>./train/8c0klk5ule.jpg</td>\n",
       "      <td>5.599206</td>\n",
       "      <td>i am an aries and just flat out liked this ide...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74568 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         img_name                img_path       mos  \\\n",
       "0      41wy7upxzl  ./train/41wy7upxzl.jpg  5.569231   \n",
       "1      ygujjq6xxt  ./train/ygujjq6xxt.jpg  6.103175   \n",
       "2      wk321130q0  ./train/wk321130q0.jpg  5.541985   \n",
       "3      w50dp2zjpg  ./train/w50dp2zjpg.jpg  6.234848   \n",
       "4      l7rqfxeuh0  ./train/l7rqfxeuh0.jpg  5.190476   \n",
       "...           ...                     ...       ...   \n",
       "74563  zbevd0lyox  ./train/zbevd0lyox.jpg  5.926108   \n",
       "74564  w26yu6ee60  ./train/w26yu6ee60.jpg  5.966346   \n",
       "74565  a1pts9zzdx  ./train/a1pts9zzdx.jpg  5.718447   \n",
       "74566  pzbubeo03l  ./train/pzbubeo03l.jpg  6.007843   \n",
       "74567  8c0klk5ule  ./train/8c0klk5ule.jpg  5.599206   \n",
       "\n",
       "                                                comments  \n",
       "0      the pink and blue really compliment each other...  \n",
       "1                            love rhubarb! great colors!  \n",
       "2      i enjoy the textures and grungy feel to this. ...  \n",
       "3      i like all the different colours in this pic, ...  \n",
       "4      i love these critters, just wish he was a litt...  \n",
       "...                                                  ...  \n",
       "74563   perfect balance here, in this soft serene image.  \n",
       "74564  very nice indeed. the sharpness and contrast a...  \n",
       "74565                  nice tones and color for balance.  \n",
       "74566          i like the bold colors. nice sharp image.  \n",
       "74567  i am an aries and just flat out liked this ide...  \n",
       "\n",
       "[74568 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb969079-ac51-4a58-a7ac-20d27486022a",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MANIQA2transformer(\n",
       "  (cnn): MANIQA(\n",
       "    (vit): VisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(8, 8), stride=(8, 8))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (patch_drop): Identity()\n",
       "      (norm_pre): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (6): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (7): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (8): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (9): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (10): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (11): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (fc_norm): Identity()\n",
       "      (head_drop): Dropout(p=0.0, inplace=False)\n",
       "      (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       "    )\n",
       "    (tablock1): ModuleList(\n",
       "      (0-1): 2 x TABlock(\n",
       "        (c_q): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (c_k): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (c_v): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (conv1): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (swintransformer1): SwinTransformer(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (downsample): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (layers): ModuleList(\n",
       "        (0): BasicLayer(\n",
       "          dim=768, input_resolution=(28, 28), depth=2\n",
       "          (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinBlock(\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                dim=768, window_size=(4, 4), num_heads=4\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinBlock(\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                dim=768, window_size=(4, 4), num_heads=4\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.033)\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): BasicLayer(\n",
       "          dim=768, input_resolution=(28, 28), depth=2\n",
       "          (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinBlock(\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                dim=768, window_size=(4, 4), num_heads=4\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.067)\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinBlock(\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                dim=768, window_size=(4, 4), num_heads=4\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.100)\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (tablock2): ModuleList(\n",
       "      (0-1): 2 x TABlock(\n",
       "        (c_q): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (c_k): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (c_v): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (conv2): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (swintransformer2): SwinTransformer(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (downsample): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (layers): ModuleList(\n",
       "        (0): BasicLayer(\n",
       "          dim=384, input_resolution=(28, 28), depth=2\n",
       "          (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinBlock(\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                dim=384, window_size=(4, 4), num_heads=4\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=768, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=768, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinBlock(\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                dim=384, window_size=(4, 4), num_heads=4\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.033)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=768, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=768, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): BasicLayer(\n",
       "          dim=384, input_resolution=(28, 28), depth=2\n",
       "          (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinBlock(\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                dim=384, window_size=(4, 4), num_heads=4\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.067)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=768, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=768, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinBlock(\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                dim=384, window_size=(4, 4), num_heads=4\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.100)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=768, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=768, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (scoring): quality_scoring(\n",
       "    (fc_score): Sequential(\n",
       "      (0): Linear(in_features=384, out_features=192, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): Linear(in_features=192, out_features=1, bias=True)\n",
       "      (4): ReLU()\n",
       "    )\n",
       "    (fc_weight): Sequential(\n",
       "      (0): Linear(in_features=384, out_features=192, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): Linear(in_features=192, out_features=1, bias=True)\n",
       "      (4): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       "  (dropout_2): Dropout(p=0.3, inplace=False)\n",
       "  (selu): SELU()\n",
       "  (embedding): Embedding(25000, 512)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_e): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=896, out_features=896, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=896, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=896, bias=True)\n",
       "        (norm1): LayerNorm((896,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((896,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mapping_decoder): Linear(in_features=896, out_features=512, bias=True)\n",
       "  (mapping_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (transformer_d): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=25000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_vocab = 25000\n",
    "model = MANIQA2transformer(example_vocab)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76bfd333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show_image(img, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    \n",
    "    #unnormalize \n",
    "    img[0] = img[0] * 0.229\n",
    "    img[1] = img[1] * 0.224 \n",
    "    img[2] = img[2] * 0.225 \n",
    "    img[0] += 0.485 \n",
    "    img[1] += 0.456 \n",
    "    img[2] += 0.406\n",
    "    \n",
    "    img = img.detach().cpu().numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    \n",
    "    plt.imshow(img)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02f16323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, image):\n",
    "    image = image.unsqueeze(0).cuda()\n",
    "    mos, _ = model(image)\n",
    "    return mos.item(), _"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbbfc72-534d-46d7-b63e-56d28a43b04e",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddb9e36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59654, 4) (14914, 4) (59654,) (14914,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_df, train_df.mos, test_size=0.2,shuffle=True,random_state=True)\n",
    "print(x_train.shape, x_valid.shape, y_train.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a36b9f43-93c5-4c1f-bb4f-e3dae2392e3d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 7457/7457 [1:12:39<00:00,  1.71it/s, cm_loss=2.35, loss=3.01, mos_loss=0.659] \n",
      "Epoch 1: 100%|██████████| 933/933 [06:43<00:00,  2.31it/s, cm_loss=2.32, loss=2.75, mos_loss=0.429] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 total train loss: 4.9592 valid loss: 2.5877\n",
      "Epoch 1 Mos train loss: 1.0328 valid loss: 0.4086\n",
      "Epoch 1 comments train loss: 3.9263 valid loss: 2.1791\n",
      "valid loss decreased inf ---> 2.5877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 7457/7457 [1:12:28<00:00,  1.71it/s, cm_loss=1.06, loss=1.44, mos_loss=0.381]   \n",
      "Epoch 2: 100%|██████████| 933/933 [06:50<00:00,  2.27it/s, cm_loss=1.11, loss=1.73, mos_loss=0.615]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 total train loss: 1.7378 valid loss: 1.4893\n",
      "Epoch 2 Mos train loss: 0.3586 valid loss: 0.3758\n",
      "Epoch 2 comments train loss: 1.3792 valid loss: 1.1136\n",
      "valid loss decreased 2.5877 ---> 1.4893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 7457/7457 [1:12:21<00:00,  1.72it/s, cm_loss=0.933, loss=1.07, mos_loss=0.138]  \n",
      "Epoch 3: 100%|██████████| 933/933 [06:47<00:00,  2.29it/s, cm_loss=0.807, loss=0.914, mos_loss=0.107] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 total train loss: 1.0787 valid loss: 1.1137\n",
      "Epoch 3 Mos train loss: 0.2407 valid loss: 0.2743\n",
      "Epoch 3 comments train loss: 0.8381 valid loss: 0.8394\n",
      "valid loss decreased 1.4893 ---> 1.1137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 7457/7457 [1:11:50<00:00,  1.73it/s, cm_loss=0.974, loss=1.05, mos_loss=0.0809] \n",
      "Epoch 4: 100%|██████████| 933/933 [06:41<00:00,  2.32it/s, cm_loss=0.728, loss=0.79, mos_loss=0.0617] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 total train loss: 0.7855 valid loss: 0.9557\n",
      "Epoch 4 Mos train loss: 0.1556 valid loss: 0.2533\n",
      "Epoch 4 comments train loss: 0.6298 valid loss: 0.7024\n",
      "valid loss decreased 1.1137 ---> 0.9557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 7457/7457 [1:11:54<00:00,  1.73it/s, cm_loss=0.369, loss=0.417, mos_loss=0.0477]   \n",
      "Epoch 5: 100%|██████████| 933/933 [06:46<00:00,  2.30it/s, cm_loss=0.687, loss=0.928, mos_loss=0.241] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 total train loss: 0.6161 valid loss: 0.8729\n",
      "Epoch 5 Mos train loss: 0.1104 valid loss: 0.2476\n",
      "Epoch 5 comments train loss: 0.5057 valid loss: 0.6253\n",
      "valid loss decreased 0.9557 ---> 0.8729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 7457/7457 [1:12:16<00:00,  1.72it/s, cm_loss=0.286, loss=0.335, mos_loss=0.049]   \n",
      "Epoch 6: 100%|██████████| 933/933 [06:46<00:00,  2.30it/s, cm_loss=0.64, loss=0.718, mos_loss=0.078]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 total train loss: 0.5039 valid loss: 0.8134\n",
      "Epoch 6 Mos train loss: 0.0843 valid loss: 0.2366\n",
      "Epoch 6 comments train loss: 0.4195 valid loss: 0.5768\n",
      "valid loss decreased 0.8729 ---> 0.8134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 7457/7457 [1:13:00<00:00,  1.70it/s, cm_loss=0.251, loss=0.349, mos_loss=0.0983]    \n",
      "Epoch 7: 100%|██████████| 933/933 [06:54<00:00,  2.25it/s, cm_loss=0.606, loss=0.626, mos_loss=0.0206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 total train loss: 0.4235 valid loss: 0.7746\n",
      "Epoch 7 Mos train loss: 0.0710 valid loss: 0.2258\n",
      "Epoch 7 comments train loss: 0.3525 valid loss: 0.5488\n",
      "valid loss decreased 0.8134 ---> 0.7746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 7457/7457 [1:12:43<00:00,  1.71it/s, cm_loss=0.578, loss=0.654, mos_loss=0.076]     \n",
      "Epoch 8: 100%|██████████| 933/933 [06:46<00:00,  2.30it/s, cm_loss=0.575, loss=0.579, mos_loss=0.00427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 total train loss: 0.3576 valid loss: 0.7464\n",
      "Epoch 8 Mos train loss: 0.0609 valid loss: 0.2267\n",
      "Epoch 8 comments train loss: 0.2967 valid loss: 0.5197\n",
      "valid loss decreased 0.7746 ---> 0.7464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 7457/7457 [1:12:03<00:00,  1.72it/s, cm_loss=0.363, loss=0.383, mos_loss=0.0206]    \n",
      "Epoch 9: 100%|██████████| 933/933 [06:45<00:00,  2.30it/s, cm_loss=0.553, loss=0.558, mos_loss=0.0053]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 total train loss: 0.3034 valid loss: 0.7314\n",
      "Epoch 9 Mos train loss: 0.0550 valid loss: 0.2227\n",
      "Epoch 9 comments train loss: 0.2484 valid loss: 0.5087\n",
      "valid loss decreased 0.7464 ---> 0.7314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 7457/7457 [1:11:19<00:00,  1.74it/s, cm_loss=0.138, loss=0.177, mos_loss=0.0388]    \n",
      "Epoch 10: 100%|██████████| 933/933 [06:42<00:00,  2.32it/s, cm_loss=0.523, loss=0.525, mos_loss=0.00168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 total train loss: 0.2563 valid loss: 0.7301\n",
      "Epoch 10 Mos train loss: 0.0499 valid loss: 0.2256\n",
      "Epoch 10 comments train loss: 0.2064 valid loss: 0.5045\n",
      "valid loss decreased 0.7314 ---> 0.7301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 7457/7457 [1:11:11<00:00,  1.75it/s, cm_loss=0.174, loss=0.192, mos_loss=0.018]     \n",
      "Epoch 11: 100%|██████████| 933/933 [06:41<00:00,  2.32it/s, cm_loss=0.506, loss=0.566, mos_loss=0.06]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 total train loss: 0.2136 valid loss: 0.7058\n",
      "Epoch 11 Mos train loss: 0.0447 valid loss: 0.2182\n",
      "Epoch 11 comments train loss: 0.1689 valid loss: 0.4876\n",
      "valid loss decreased 0.7301 ---> 0.7058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 7457/7457 [1:11:33<00:00,  1.74it/s, cm_loss=0.323, loss=0.364, mos_loss=0.0407]    \n",
      "Epoch 12: 100%|██████████| 933/933 [06:45<00:00,  2.30it/s, cm_loss=0.463, loss=0.491, mos_loss=0.0275] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 total train loss: 0.1769 valid loss: 0.6950\n",
      "Epoch 12 Mos train loss: 0.0420 valid loss: 0.2302\n",
      "Epoch 12 comments train loss: 0.1350 valid loss: 0.4649\n",
      "valid loss decreased 0.7058 ---> 0.6950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 7457/7457 [1:12:05<00:00,  1.72it/s, cm_loss=0.0464, loss=0.0939, mos_loss=0.0475]   \n",
      "Epoch 13: 100%|██████████| 933/933 [06:45<00:00,  2.30it/s, cm_loss=0.477, loss=0.481, mos_loss=0.00426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 total train loss: 0.1438 valid loss: 0.6760\n",
      "Epoch 13 Mos train loss: 0.0390 valid loss: 0.2140\n",
      "Epoch 13 comments train loss: 0.1048 valid loss: 0.4620\n",
      "valid loss decreased 0.6950 ---> 0.6760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 7457/7457 [1:12:02<00:00,  1.72it/s, cm_loss=0.0304, loss=0.0598, mos_loss=0.0294]    \n",
      "Epoch 14: 100%|██████████| 933/933 [06:46<00:00,  2.30it/s, cm_loss=0.393, loss=0.41, mos_loss=0.0165]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 total train loss: 0.1141 valid loss: 0.6718\n",
      "Epoch 14 Mos train loss: 0.0359 valid loss: 0.2175\n",
      "Epoch 14 comments train loss: 0.0782 valid loss: 0.4543\n",
      "valid loss decreased 0.6760 ---> 0.6718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 7457/7457 [1:12:06<00:00,  1.72it/s, cm_loss=0.13, loss=0.152, mos_loss=0.0216]       \n",
      "Epoch 15: 100%|██████████| 933/933 [06:45<00:00,  2.30it/s, cm_loss=0.411, loss=0.417, mos_loss=0.00532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 total train loss: 0.0897 valid loss: 0.6624\n",
      "Epoch 15 Mos train loss: 0.0343 valid loss: 0.2131\n",
      "Epoch 15 comments train loss: 0.0555 valid loss: 0.4493\n",
      "valid loss decreased 0.6718 ---> 0.6624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16:  59%|█████▉    | 4396/7457 [42:30<29:36,  1.72it/s, cm_loss=0.0891, loss=0.178, mos_loss=0.0885]     \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lee junseok\\my_project\\Samsung\\image captioning\\ViT+transformer.ipynb Cell 23\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lee%20junseok/my_project/Samsung/image%20captioning/ViT%2Btransformer.ipynb#X30sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lee%20junseok/my_project/Samsung/image%20captioning/ViT%2Btransformer.ipynb#X30sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lee%20junseok/my_project/Samsung/image%20captioning/ViT%2Btransformer.ipynb#X30sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lee%20junseok/my_project/Samsung/image%20captioning/ViT%2Btransformer.ipynb#X30sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m train_mos_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss1\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lee%20junseok/my_project/Samsung/image%20captioning/ViT%2Btransformer.ipynb#X30sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m train_cm_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss2\u001b[39m.\u001b[39mitem()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "train_data = pd.read_csv('train.csv')\n",
    "\n",
    "# 단어 사전 생성\n",
    "all_comments = ' '.join(train_data['comments']).split()\n",
    "vocab = set(all_comments)\n",
    "vocab = ['<PAD>', '<SOS>', '<EOS>'] + list(vocab)\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "train_dataset = CustomDataset(x_train)\n",
    "valid_dataset = CustomDataset(x_valid)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "model = MANIQA2transformer(len(vocab))\n",
    "\n",
    "\n",
    "model.cuda()\n",
    "criterion1 = nn.MSELoss()\n",
    "criterion2 = nn.CrossEntropyLoss(ignore_index=word2idx['<PAD>'])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5,weight_decay=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100,eta_min=0)\n",
    "print_every = 100\n",
    "\n",
    "# 학습\n",
    "model.train()\n",
    "valid_min_loss = np.Inf\n",
    "for epoch in range(100):\n",
    "    train_loss = 0\n",
    "    train_mos_loss = 0\n",
    "    train_cm_loss = 0\n",
    "    valid_loss = 0\n",
    "    valid_mos_loss = 0\n",
    "    valid_cm_loss = 0\n",
    "    train_loop = tqdm(train_loader, leave=True)\n",
    "    for imgs, mos, comments in train_loop:\n",
    "        imgs, mos = imgs.float().cuda(), mos.float().cuda()\n",
    "        \n",
    "        # Batch Preprocessing\n",
    "        src_tensor = torch.zeros((len(comments), len(max(comments, key=len)))).long().cuda()\n",
    "        for i, comment in enumerate(comments):\n",
    "            tokenized = ['<SOS>'] + comment.split() + ['<EOS>']\n",
    "            src_tensor[i, :len(tokenized)] = torch.tensor([word2idx[word] for word in tokenized])\n",
    "        \n",
    "        \n",
    "        tgt = src_tensor[:,1:]\n",
    "        sequence_len = src_tensor[:,1:].size(1)\n",
    "        tgt_mask = model.get_tgt_mask(sequence_len).cuda()\n",
    "        # Forward & Loss\n",
    "        predicted_mos, predicted_comments = model(imgs, src_tensor[:,:-1],tgt, tgt_mask)\n",
    "        loss1 = criterion1(predicted_mos, mos)\n",
    "        loss2 = criterion2(predicted_comments.view(-1, len(vocab)), tgt.reshape(-1))\n",
    "        loss = loss1 + loss2\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_mos_loss += loss1.item()\n",
    "        train_cm_loss += loss2.item()\n",
    "        train_loop.set_description(f\"Epoch {epoch + 1}\")\n",
    "        train_loop.set_postfix(loss=loss.item(),mos_loss=loss1.item(),cm_loss=loss2.item())\n",
    "\n",
    "    valid_loop = tqdm(valid_loader, leave=True)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for imgs, mos, comments in valid_loop:\n",
    "            imgs, mos = imgs.float().cuda(), mos.float().cuda()\n",
    "                \n",
    "            # Batch Preprocessing\n",
    "            src_tensor = torch.zeros((len(comments), len(max(comments, key=len)))).long().cuda()\n",
    "            for i, comment in enumerate(comments):\n",
    "                tokenized = ['<SOS>'] + comment.split() + ['<EOS>']\n",
    "                src_tensor[i, :len(tokenized)] = torch.tensor([word2idx[word] for word in tokenized])\n",
    "                \n",
    "                \n",
    "            tgt = src_tensor[:,1:]\n",
    "            sequence_len = src_tensor[:,1:].size(1)\n",
    "            tgt_mask = model.get_tgt_mask(sequence_len).cuda()\n",
    "            # Forward & Loss\n",
    "            predicted_mos, predicted_comments = model(imgs, src_tensor[:,:-1],tgt, tgt_mask)\n",
    "            loss1 = criterion1(predicted_mos, mos)\n",
    "            loss2 = criterion2(predicted_comments.view(-1, len(vocab)), tgt.reshape(-1))\n",
    "            loss = loss1 + loss2\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "            valid_mos_loss += loss1.item()\n",
    "            valid_cm_loss += loss2.item()\n",
    "            valid_loop.set_description(f\"Epoch {epoch + 1}\")\n",
    "            valid_loop.set_postfix(loss=loss.item(),mos_loss=loss1.item(),cm_loss=loss2.item())\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} total train loss: {train_loss / len(train_loader):.4f} valid loss: {valid_loss / len(valid_loader):.4f}\")\n",
    "    print(f\"Epoch {epoch + 1} Mos train loss: {train_mos_loss / len(train_loader):.4f} valid loss: {valid_mos_loss / len(valid_loader):.4f}\")\n",
    "    print(f\"Epoch {epoch + 1} comments train loss: {train_cm_loss / len(train_loader):.4f} valid loss: {valid_cm_loss / len(valid_loader):.4f}\")\n",
    "    if valid_min_loss > valid_loss / len(valid_loader):\n",
    "       print('valid loss decreased {:.4f} ---> {:.4f}'.format(valid_min_loss,(valid_loss/len(valid_loader))))\n",
    "       valid_min_loss = valid_loss / len(valid_loader)\n",
    "       torch.save(model.state_dict(),'MANIAQA+transformer.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864072e8-85dc-435d-9198-5b9e1f61bd24",
   "metadata": {},
   "source": [
    "## Inference & Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e881eb92-a172-479e-92e2-38f852a488e5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BaseModel(vocab_size=len(vocab)).cuda()\n",
    "model.load_state_dict(torch.load('MANIQA+transformer.pt'))\n",
    "test_data = pd.read_csv('test.csv')\n",
    "test_dataset = CustomDataset(test_data)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "predicted_mos_list = []\n",
    "predicted_comments_list = []\n",
    "\n",
    "def greedy_decode(model, image):\n",
    "    image = image.unsqueeze(0).cuda()\n",
    "    mos, _ = model(image)\n",
    "    return mos.item(), _\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for imgs, _, _ in tqdm(test_loader):\n",
    "        for img in imgs:\n",
    "            img = img.float().cuda()\n",
    "            mos, temp = greedy_decode(model, img)\n",
    "\n",
    "            features = model.cnn(img.unsqueeze(0))\n",
    "\n",
    "            caps= model.generate_caption(features)\n",
    "            caption = ' '.join(caps)\n",
    "            predicted_mos_list.append(mos)\n",
    "            predicted_comments_list.append(caption)\n",
    "            # show_image(imgs[0],title=caption)\n",
    "\n",
    "# 결과 저장\n",
    "result_df = pd.DataFrame({\n",
    "    'img_name': test_data['img_name'],\n",
    "    'mos': predicted_mos_list,\n",
    "    'comments': predicted_comments_list  # 캡션 부분은 위에서 생성한 것을 사용\n",
    "})\n",
    "\n",
    "# 예측 결과에 NaN이 있다면, 제출 시 오류가 발생하므로 후처리 진행 (sample_submission.csv과 동일하게)\n",
    "result_df['comments'] = result_df['comments'].fillna('Nice Image.')\n",
    "result_df.to_csv('submit.csv', index=False)\n",
    "\n",
    "print(\"Inference completed and results saved to submit.csv.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.14 (NGC 22.12/Python 3.8) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
